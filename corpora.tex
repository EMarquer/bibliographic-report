\chapter{Corpora used}

\section{The original corpora: the industrial dataset from the PAPUD project}
The data from the PAPUD project is given to us by industrial partners of the project (BULL-ATOS). It is a set of system log files from their servers, which in total amount to more than 400GiB of compressed data; we are currently using a small subset of this data for our model.

The data from that dataset is confidential. Thus, the example we present in \autoref{tab:indu} has been altered.


\begin{figure}[!h]
	\begin{center}
		
		\begin{tabularx}{.9\textwidth}{|X|X|}
			\hline 
			Timestamp & 1524463200\\
			Date &2018 Apr 23\\
			Time &08:00:00\\
			User&OOO\\
			Process &authpriv\\
			Message type&info\\
			Message& access granted for user root (uid=0)\\ 
			\hline 
			\multicolumn{2}{|c|}{1524463200 2018 Apr 23 08:00:00 OOO authpriv info acce[...]ot (uid=0)} \\ 
			\hline 
		\end{tabularx} 
	\end{center}
	%légende de l'image
	\caption{Log line example from the industrial dataset (the full line has been abbreviated) \label{tab:indu}}
\end{figure}

\subsection{Preprocessing}
As mentioned in \autoref{sec:preprocess} (\autopageref{sec:preprocess}), we try to do as little preprocessing on the data as possible. However, for the industrial data, our preprocessing (which was designed in a previous part of the project on the same data), is non-negligible.
\newpage
The complete preprocessing is as follows:
\begin{enumerate}
	\item we remove the Timestamp, Date, Time, and User elements, as they are really redundant and do not interest us for now;
	\item we replace hexadecimal numbers/memory addresses by a specific character depending on the structure of the hexadecimal number;
	\item we normalize all the lines to a length of 200 characters, by removing the excess characters and padding the shorter lines with a specific padding character;
	\item we map of all the characters to numbers through an automatically generated dictionary.
\end{enumerate}

\section{Additional corpora: the LANL dataset}
The other dataset we are using has be generated by Los Alamos National Library (LANL). This is a publicly available dataset \cite{lanl_source}, which contains around one billion log lines which is generated over the span of 58 consecutive days. The logs are about anonymized processes, network flow, DNS and authentication information \cite{rnn_attention_lanl}. Interleaved are attacks by their Red Team \cite{rnn_attention_lanl}.
Our project work on the log lines which have the specific format which is the following:

\begin{center}
Source user, Destination user, Source pc, Destination pc, Authentication type, Logon type, Authentication orientation, Success/failure
\end{center}

An example from the article \cite{rnn_attention_lanl} is presented in \autoref{tab:lanl}.

\begin{figure}[!h]
\begin{center}

\begin{tabularx}{.9\textwidth}{|X|X|}
	\hline 
	Timestamp & 1\\
	Source user &C6@D1\\
	Destination user &U7@D2\\
	Source PC&C6\\
	Destination PC&C6\\
	Authentication type&Negotiate\\
	Logon type&Batch\\
	Autentication orientation&LogOn\\
	Success/Failure& Success\\ 
	\hline 
	\multicolumn{2}{|c|}{1,C6@D1,U7@D2,C6,C6,Negotiate,Batch,LogOn,Success} \\ 
	\hline 
\end{tabularx} 
\end{center}
%légende de l'image
\caption{Log line example from the LANL dataset \cite{rnn_attention_lanl}\label{tab:lanl}}
\end{figure} 
These log lines have been generated from desktop PC and active directory servers which are using Windows OS. In this dataset they have discarded all the lines that have machine listed as source user \cite{rnn_attention_lanl}.

We use this dataset to test our model on intermediate- to large-size training dataset, with a simpler structure than the industrial dataset we got, with two objectives in mine: to use a simpler dataset to improve our model, and to compare the performance of our model on the LANL and industrial dataset.

\subsection{Preprocessing}
As mentioned in \autoref{sec:preprocess} (\autopageref{sec:preprocess}), we do very little preprocessing on the data from the LANL database, with only tree very simple process:
\begin{enumerate}
	\item we remove the Timestamp field;
	\item we normalize all the lines to a length of 126 characters, by removing the excess characters and padding the shorter lines with a specific padding character;
	\item we map of all the characters to numbers through an automatically generated dictionary.
\end{enumerate}

Note that in \cite{rnn_attention_lanl}, they keep the timestamp information in the log line. However, we do not use them in our model as they do not interest us for now, and due to a consistency concern with the processing we do on the industrial dataset.
